{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural network for digit localization and classification\n",
    "Input is an image of a number, output is the number on the image\n",
    "Framework used: PyTorch\n",
    "For classification, we use a LeNet CNN architecture, trained on MNIST dataset\n",
    "For localization, we use a simple CNN architecture, trained on dataset composed of images from MNIST dataset, with bounding boxes around digits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as tfs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9912422 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bfaa92c36074c74830bfb0dd0217c79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/28881 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1827cb4a33e4441ea76d6897104fb16d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49bdbbff79d945da9ee291d967442a98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cc279d101454f089c6027db38828fce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Load MNIST dataset\n",
    "data_tfs = tfs.Compose([\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "# install for train and test\n",
    "root = './'\n",
    "train_dataset = MNIST(root, train=True,  transform=data_tfs, download=True)\n",
    "val_dataset  = MNIST(root, train=False, transform=data_tfs, download=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define LeNet CNN architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model_cls = LeNet().to(device)\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = torch.optim.Adam(model_cls.parameters())\n",
    "\n",
    "loaders = {\"train\": train_dataloader, \"valid\": valid_dataloader}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Train LeNet CNN\n",
    "def train(model, criterion, optimizer, loaders, epochs=10, device=\"cpu\"):\n",
    "    accuracy = {\"train\": [], \"valid\": []}\n",
    "    for epoch in range(epochs):\n",
    "        for k, dataloader in loaders.items():\n",
    "            epoch_correct = 0\n",
    "            epoch_all = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                if k == \"train\":\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    outp = model(x_batch)\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        outp = model(x_batch)\n",
    "\n",
    "                preds = outp.argmax(-1)\n",
    "                correct = (preds == y_batch).sum()\n",
    "                all = len(preds)\n",
    "                epoch_correct += correct.item()\n",
    "                epoch_all += all\n",
    "                if k == \"train\":\n",
    "                    loss = criterion(outp, y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            if k == \"train\":\n",
    "                print(f\"Epoch: {epoch+1}\")\n",
    "            print(f\"Loader: {k}. Accuracy: {epoch_correct/epoch_all}\")\n",
    "            accuracy[k].append(epoch_correct/epoch_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loader: train. Accuracy: 0.8733333333333333\n",
      "Loader: valid. Accuracy: 0.9574\n",
      "Epoch: 2\n",
      "Loader: train. Accuracy: 0.9645333333333334\n",
      "Loader: valid. Accuracy: 0.9718\n",
      "Epoch: 3\n",
      "Loader: train. Accuracy: 0.9759666666666666\n",
      "Loader: valid. Accuracy: 0.9771\n",
      "Epoch: 4\n",
      "Loader: train. Accuracy: 0.9812166666666666\n",
      "Loader: valid. Accuracy: 0.9834\n",
      "Epoch: 5\n",
      "Loader: train. Accuracy: 0.9844\n",
      "Loader: valid. Accuracy: 0.9836\n",
      "Epoch: 6\n",
      "Loader: train. Accuracy: 0.9866833333333334\n",
      "Loader: valid. Accuracy: 0.9841\n",
      "Epoch: 7\n",
      "Loader: train. Accuracy: 0.9887666666666667\n",
      "Loader: valid. Accuracy: 0.9853\n",
      "Epoch: 8\n",
      "Loader: train. Accuracy: 0.9898833333333333\n",
      "Loader: valid. Accuracy: 0.9861\n",
      "Epoch: 9\n",
      "Loader: train. Accuracy: 0.99115\n",
      "Loader: valid. Accuracy: 0.9869\n",
      "Epoch: 10\n",
      "Loader: train. Accuracy: 0.9920333333333333\n",
      "Loader: valid. Accuracy: 0.9859\n"
     ]
    }
   ],
   "source": [
    "train(model_cls, criterion_cls, optimizer_cls, loaders, epochs=10, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model_cls.state_dict(), \"model_cls.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Create dataset for digit localization\n",
    "def create_localization_dataset(dataset):\n",
    "    if not os.path.exists(\"localization_dataset\"):\n",
    "        os.mkdir(\"localization_dataset\")\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        img, label = train_dataset[i]\n",
    "        img = img.squeeze().numpy()\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "        _, thresh = cv2.threshold(img, 127, 255, 0)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt = contours[0]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        yield i, x, y, w, h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "loc_train_dataset = list(create_localization_dataset(train_dataset))\n",
    "loc_val_dataset = list(create_localization_dataset(val_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "pd.DataFrame(loc_train_dataset).to_csv(\"loc_train_dataset.csv\", index=False)\n",
    "pd.DataFrame(loc_val_dataset).to_csv(\"loc_val_dataset.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "loc_train_dataset = pd.read_csv(\"loc_train_dataset.csv\").values\n",
    "loc_val_dataset = pd.read_csv(\"loc_val_dataset.csv\").values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def split_xy(dataset, img_dataset):\n",
    "    new_dataset = []\n",
    "    for i, x, y, w, h in dataset:\n",
    "        new_dataset.append((img_dataset[i][0], (x, y, w, h)))\n",
    "    return new_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "loc_train_dataset = split_xy(loc_train_dataset, train_dataset)\n",
    "loc_val_dataset = split_xy(loc_val_dataset, val_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765, -0.8588,\n           -0.8588, -0.8588, -0.0118,  0.0667,  0.3725, -0.7961,  0.3020,\n            1.0000,  0.9373, -0.0039, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -0.7647, -0.7176, -0.2627,  0.2078,  0.3333,  0.9843,\n            0.9843,  0.9843,  0.9843,  0.9843,  0.7647,  0.3490,  0.9843,\n            0.8980,  0.5294, -0.4980, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -0.6157,  0.8667,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n            0.9843,  0.9843,  0.9843,  0.9686, -0.2706, -0.3569, -0.3569,\n           -0.5608, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -0.8588,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n            0.5529,  0.4275,  0.9373,  0.8902, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -0.3725,  0.2235, -0.1608,  0.9843,  0.9843,  0.6078,\n           -0.9137, -1.0000, -0.6627,  0.2078, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -0.8902, -0.9922,  0.2078,  0.9843, -0.2941,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000,  0.0902,  0.9843,  0.4902,\n           -0.9843, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -0.9137,  0.4902,  0.9843,\n           -0.4510, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7255,  0.8902,\n            0.7647,  0.2549, -0.1529, -0.9922, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3647,\n            0.8824,  0.9843,  0.9843, -0.0667, -0.8039, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -0.6471,  0.4588,  0.9843,  0.9843,  0.1765, -0.7882, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -0.8745, -0.2706,  0.9765,  0.9843,  0.4667, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000,  0.9529,  0.9843,  0.9529, -0.4980,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -0.6392,  0.0196,  0.4353,  0.9843,  0.9843,  0.6235, -0.9843,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6941,  0.1608,\n            0.7961,  0.9843,  0.9843,  0.9843,  0.9608,  0.4275, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -0.8118, -0.1059,  0.7333,  0.9843,\n            0.9843,  0.9843,  0.9843,  0.5765, -0.3882, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -0.8196, -0.4824,  0.6706,  0.9843,  0.9843,  0.9843,\n            0.9843,  0.5529, -0.3647, -0.9843, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,\n            0.3412,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.5294,\n           -0.3725, -0.9294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -0.5686,  0.3490,  0.7725,\n            0.9843,  0.9843,  0.9843,  0.9843,  0.9137,  0.0431, -0.9137,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000,  0.0667,  0.9843,  0.9843,\n            0.9843,  0.6627,  0.0588,  0.0353, -0.8745, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]),\n (5, 5, 19, 20))"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "loc_train_dataloader = DataLoader(loc_train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "loc_valid_dataloader = DataLoader(loc_val_dataset, batch_size=128, shuffle=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Define CNN architecture for digit localization for images of size 140x140\n",
    "class LocalizationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LocalizationNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "model_loc = LocalizationNet().to(device)\n",
    "\n",
    "criterion_loc = nn.MSELoss()\n",
    "optimizer_loc = torch.optim.Adam(model_loc.parameters())\n",
    "\n",
    "loaders = {\"train\": loc_train_dataloader, \"valid\": loc_valid_dataloader}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Train CNN for digit localization\n",
    "def train_loc(model, criterion, optimizer, loaders, epochs=10, device=\"cpu\"):\n",
    "    for epoch in range(epochs):\n",
    "        for k, dataloader in loaders.items():\n",
    "            epoch_loss = 0\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                if k == \"train\":\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "                    outp = model(x_batch)\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        outp = model(x_batch)\n",
    "\n",
    "                loss = criterion(outp, y_batch)\n",
    "                epoch_loss += loss.item()\n",
    "                if k == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            if k == \"train\":\n",
    "                print(f\"Epoch: {epoch+1}\")\n",
    "            print(f\"Loader: {k}. Loss: {epoch_loss/len(dataloader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [54], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_loc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion_loc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_loc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [53], line 7\u001B[0m, in \u001B[0;36mtrain_loc\u001B[1;34m(model, criterion, optimizer, loaders, epochs, device)\u001B[0m\n\u001B[0;32m      5\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m----> 7\u001B[0m     x_batch, y_batch \u001B[38;5;241m=\u001B[39m x_batch\u001B[38;5;241m.\u001B[39mto(device), \u001B[43my_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m(device)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m      9\u001B[0m         model\u001B[38;5;241m.\u001B[39mtrain()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "train_loc(model_loc, criterion_loc, optimizer_loc, loaders, epochs=10, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model_loc.state_dict(), \"model_loc.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define function to predict\n",
    "def predict(model_cls, model_loc, img, device=\"cpu\"):\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    model_cls.eval()\n",
    "    with torch.no_grad():\n",
    "        outp_cls = model_cls(img)\n",
    "    preds_cls = outp_cls.argmax(-1)\n",
    "    model_loc.eval()\n",
    "    with torch.no_grad():\n",
    "        outp_loc = model_loc(img)\n",
    "    x, y, w, h = outp_loc.squeeze().cpu().numpy()\n",
    "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "    return preds_cls.item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}